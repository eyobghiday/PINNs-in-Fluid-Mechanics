This methodology involves generating a capable PINN model for the 2D-Difussion equation using a numerical method, and tensor flows. To begin, the diffusion equation is a partial differential equation that describes the spread of a quantity over time, given its initial distribution. In 2D, the diffusion equation is will be derived using the Green's functions \cite{Srivastava_2022} and linear operator PDE's.  The boundary conditions have to be specified,  in order to solve the diffusion equation. In this project both the Dirichlet's and Neu-
mann's boundary conditions will be utilised \cite{Willis1980}. A neural network architecture is then chosen to approximate the solution to the diffusion equation. The neural network takes in the input variables $(x, y, t)$ and outputs the predicted value of $u(x,y,t)$ as an example. The architecture can be designed based on the complexity of the problem and the available data. Following this we can use the PINNs technique to train the neural network. The PINN technique involves minimizing the difference between the predicted and actual values of the quantity of interest, as well as satisfying the differential equation and boundary conditions. This can be done using optimization algorithms such as stochastic gradient descent, Sequential or Adam \cite{FernandezdelaMata2023}. Finally, after the neural network is trained, we can use it to predict the distribution of $u(x,y,t)$ function over time. We can then use data visualization techniques such as contour plots or surface plots to visualize the predicted distribution. 

During training, the PINN model should satisfy the 2D Diffusion equations as a constraint, and the generated data set should be used to compute the loss function. As a result the performance of the PINN model will be evaluated in predicting the diffusivity using various metrics, such as mean absolute error, root mean square error, and or the correlation coefficient.

\begin{figure}[htb!]
\begin{center}
\includegraphics[width=.49\textwidth]{images/arc.pdf}
\vspace*{-8mm}
\caption{PINNs Architecture}
\label{fig:arch}
\end{center}
\end{figure}

\subsection{Solution to the 2D Diffusion Equation}

The 2D diffusion equation allows us to talk about the statistical movements of randomly moving particles in two dimensions. The movement of each individual particle does not follow the equation, but many identical particles each obeying the same boundary and initial conditions share some statistical properties. In this derivation of the diffusion PDE we well use function $P$ instead of $U$ to easily set apart the derivation function from the function in the code section even though they mean the same. 
In an ideal world the diffusion function is just the probability distribution $P(x,y,t)$ which provides the probability of finding a perfectly average particle in the small vicinity of the point (x,y) at a given time $t$. Brownian motion is a special case of diffusion 2D where the particles are subject to random forces that cause them to move in a random pattern \cite{Ursell2005}. The movement of particles in both diffusion 2D and Brownian motion is affected by the diffusion coefficient, which represents the degree of randomness in the movement of the particles. The evolution of some systems does follow the equation outright but as a group they exhibit the smooth, well-behaved statistical features of the diffusion equation.

Now that with all the relative background knowledge, let's transform the 2D diffusion equation in Cartesian coordinates as:
\begin{align*}
    \nabla^2 - \frac{1}{D} \frac{\partial P}{ \partial t} = 0 \\
    \frac{\partial^2 P}{ \partial^2 x^2} + \frac{\partial^2 P}{ \partial^2 y^2}  - \frac{1}{D} \frac{\partial P}{ \partial t} = 0 \tag{1}
    \label{eq1}
\end{align*}


Equation \eqref{eq1} is the equation we are interested in. And in order to solve that we will need to define the boundary conditions as follows:

\begin{align*}
   \frac{\partial^2 P}{\partial x^2} \bigg \vert_{x = \pm -\infty} = \frac{\partial^2 P}{\partial y^2} \bigg \vert_{y= \pm \infty} &= 0 \\
   \frac{\partial P}{\partial x} \bigg \vert_{x= \pm \infty} = \frac{\partial P}{\partial y} \bigg \vert_{y= \pm \infty} &= 0 \tag{B.C}
    \label{bc1}
\end{align*}

Since the function $P$ is a linear partial differential equation and separable function, we can apply an inverse 2D Fourier transform to solve the solution. Consider the following integral relations that define the 2D FT in Cartesian coordinates. We will call the function $\hat{P}$ the FT of our original function P:

\begin{align*}  
  \hat{P}(k_x,k_y,t) =  \oiint_{s} e^{-i 2\pi (k_x \cdot x + k_y \cdot y )} P(x,y,t) dx dy \\  
  P(x,y,t) = \oiint_{s} e^{-i 2\pi (k_x \cdot x + k_y \cdot y )} \hat{P}(k_x,k_y,t) dx dy \tag{2}
  \label{eq2}  
\end{align*}  

Notice the symmetry in going forward and backward in the transform Equation \eqref{eq2}. This is because switching between the normal form of the problem and what we call Fourier Space, where the problem exists after the FT, are physically identical. 

Letâ€™s examine the spatial derivatives of the diffusion equation, where we consider the second derivative to be the function of interest. We can integrate these second derivatives by parts, using u and v as  follows:
% \begin{center}
    \begin{align*}  
\int\limits_{a}^{b} udv = uv \bigg \vert^{a}_{b} - \int\limits_{a}^{b} vdu \\
u = e^{-i 2\pi (k_x \cdot x + k_y \cdot y )} \textnormal{ and } v= \frac{\partial P}{\partial x} \\
du= \frac{\partial}{\partial x} e^{-i 2\pi (k_x \cdot x + k_y \cdot y )} \textnormal{ and } dv =  \frac{\partial^2 P}{\partial x^2} dx
\label{ibp}  
\end{align*}  
% \end{center}
\twocolumn[\begin{@twocolumnfalse}

Using the substitution of integration by parts wee can set up the whole integral as follows.

\begin{align}  
% \iint\limits_{-\infty}^{\infty} e^{-i 2\pi (k_x \cdot x + k_y \cdot y )} \hat{P}(k_x,k_y,t) dx dy \tag{5} \\
\integral e^{-i 2\pi (k_x \cdot x + k_y \cdot y )} \frac{\partial^2 P}{\partial x^2} dx dy &= e^{-i 2\pi (k_x \cdot x + k_y \cdot y )} \frac{\partial P}{\partial x} \bigg \vert^{\infty}_{-\infty(x,y)} - \integral \frac{\partial }{\partial x} e^{-i 2\pi (k_x \cdot x + k_y \cdot y )} \cdot \frac{\partial P}{\partial x} dx dy\\
& = e^{-i 2\pi (k_x \cdot x + k_y \cdot y )} \frac{\partial P}{\partial x} \bigg \vert^{\infty}_{-\infty(x,y)} + i 2\pi k_x \integral e^{-i 2\pi (k_x \cdot x + k_y \cdot y )} \frac{\partial P}{\partial x} \\
& = \cancel{e^{-i 2\pi (k_x \cdot x + k_y \cdot y )} \frac{\partial P}{\partial x} \bigg \vert^{\infty}_{-\infty(x,y)}} + i 2\pi k_x \integral e^{-i 2\pi (k_x \cdot x + k_y \cdot y )} \cdot \frac{\partial P}{\partial x} dx dy\\
& = i 2\pi k_x \integral e^{-i 2\pi (k_x \cdot x + k_y \cdot y )} \frac{\partial P}{\partial x} \tag{3}
\label{eq3}  
\end{align}  

Looking the last term Equation \eqref{eq3}, we effectively transferred one derivative off $P$ and put it on the exponential of the FT, but since the exponential is an explicit function we can just perform the derivative, giving us the constant on the right most integral of the second line. We can apply similar inverse 1D Fourier transform again to get:

\begin{align}  
\integral e^{-i 2\pi (k_x \cdot x + k_y \cdot y )} \frac{\partial P}{\partial x} dx dy &= P \cdot e^{-i 2\pi (k_x \cdot x + k_y \cdot y )} \bigg \vert^{\infty}_{-\infty(x,y)} - \integral \frac{\partial }{\partial x} e^{-i 2\pi (k_x \cdot x + k_y \cdot y )} \cdot P \cdot dx dy\\
& = \cancel{P \cdot e^{-i 2\pi (k_x \cdot x + k_y \cdot y )} \bigg \vert^{\infty}_{-\infty(x,y)}} + i 2\pi k_x \integral P \cdot e^{-i 2\pi (k_x \cdot x + k_y \cdot y )} dx dy\\
& = i 2\pi k_x \integral e^{-i 2\pi (k_x \cdot x + k_y \cdot y )} dx dy \tag{4}
\label{eq4}  
\end{align} 

Equating all the L.H.S with the R.H.S Equation \eqref{eq4} we get Equation \eqref{eq5}: 

\begin{align}  
\integral e^{-i 2\pi (k_x \cdot x + k_y \cdot y )} \frac{\partial^2 P}{\partial x^2} dx dy &= (i 2\pi k_x)^2 \integral e^{-i 2\pi (k_x \cdot x + k_y \cdot y )} dx dy \\
\integral e^{-i 2\pi (k_x \cdot x + k_y \cdot y )} \frac{\partial^2 P}{\partial x^2} dx dy &= (i 2\pi k_x)^2 \hat{P} \tag{5}
\label{eq5}  
\end{align} 

Since we took the spatial FT (i.e. dealing with x and y), keep in mind, the derivative in time does not change under a FT \cite{TANG2022186}. Henceforth we can write the 2D equation \eqref{eqft} as: 

    \begin{align*}
     % \textnormal{ This generally corresopnds to the format } \\
    \frac{\partial^n}{\partia x^n} &= (i 2\pi k_x)^n \hat{P} \\
    % \textnormal{ This means the equation becomes first order ODE in time: } \\
    (2\pi)^2 \hat{P} \cdot ((k_x)^2 + (k_y)^2)  + \frac{1}{D} \frac{\partial P}{ \partial t} &= 0 \tag{6} \\
        \label{eqft}  
    \end{align*}    

Using method of separation variables we end up having an Eigen value probelm in Equation \eqref{eq11}:

\begin{align*}
    \hat{P} =\lambda e^{-D (2\pi)^2 (k_x^2 + k_y^2) \cdot t} \tag{7}
    \label{eq11}  
\end{align*}

\end{@twocolumnfalse}]

% \end{center}
\twocolumn[\begin{@twocolumnfalse}

The constant $\lambda$ is nothing but the normalization factor that can be used for the conservation of linear momentum \cite{Ursell2005}. Therefore our original function $p$ is then given by:

\begin{align*}
    P =\lambda\integral e^{-D (2\pi)^2 (k_x^2 + k_y^2) t}\cdot e^{-i 2\pi (k_x \cdot x + k_y \cdot y )}dk_x dk_y \tag{8}
    \label{eq12}  
\end{align*}

We can further compute Equation \eqref{eq12} by method of separatioin of spatial variablee as :

\begin{align*}
    P =\lambda \left\{ \int\limits^{\infty}_{\text{-}\infty} e^{-D (2\pi)^2 (k_x^2 + k_y^2) t}dk_x \right \} \cdot  \left\{ \int\limits^{\infty}_{\text{-}\infty} e^{-i 2\pi (k_x \cdotx + k_y \cdot y )}dk_y \right \} \tag{9}
    \label{eqin}  
\end{align*}

Solving Equation \eqref{eqin} by separation of integral will require completing the square of the exponent, rescaling the integration variable, changing to polar coordinates and then substituting back the Cartesian values. A detailed complete PDE derivation of the solution to the diffusion 2D problem can be found here \cite{Eyob2023}. 

By following those examples and solving we will get: 
\begin{align*}
    P(x,y,t) = \lambda \frac{e^{\frac{-(r^2)}{4 D t}}}{4 \pi D \cdot t} \\
    P(x,y,t) = \lambda \frac{e^{\frac{-(x^2 + y^2)}{4 D \cdot t}}}{4 \pi D t} \tag{10} 
    \label{eq13}  
\end{align*}
One last step is to figure out what $\lambda$ exactly is. For this sample of Pinns project teh normalization constant $\lambda$ is just the sum unity function that represents the total sum of the diffusive particles across the 2D surface in the Gaussian distribution. This means that if we apply the B.C. condition from $-\infty$ to $+\infty$ the total sum the P (x,y,t) adds up to 1. Imposing this condition leads us to the equation \eqref{eq14}: 
\begin{align*}
    \lambda = \integral P(x,y,t) dx dy= 1 \tag{11}
    \label{eq14}  
\end{align*}
There for finally the 2D diffusion equation becomes: 

\begin{align*}
    \boxed{
    P(x,y,t) = \frac{e^{\frac{-(x^2 + y^2)}{4 D \cdot t}}}{4 \pi D t} \tag{12} }
    \label{eq15}  
\end{align*}

\end{@twocolumnfalse}]

\subsection{Code Setup}
In the code we are trying to demonstrate how to use a neural network to solve a diffusion equation. The diffusion equation as utilised in Equation \eqref{eq15} is defined in the code by the function "diffusion\_equation', which takes in the position, time, and diffusion coefficient as inputs and returns the value of the equation at that point as seen in the code below. Here the $\lambda$ constant is set to 1 and the Cartesian form of the function provided in Equation \eqref{eq13} is utilised. The domain and boundary conditions for the problem are defined by creating a grid of x and t values using the NumPy meshgrid function. The model is built using the Keras Sequential API and consists of three fully connected layers, with the output layer having a linear activation function. The model is trained using mean squared error loss and the RMSprop optimizer. Finally, the model is used to predict the diffusion equation for each value of D, and the results are plotted using the Matplotlib library. Full codes to the project can be found here \cite{Eyob2023_Git}.

\begin{center}
   \begin{verbatim}
        # This is my Diffusion equation
        def diffusion_equation(x, t, D):
          return 1 / (4 * np.pi * D * t) * 
          np.exp(-np.linalg.norm(x) ** 2 / (4 * D * t))
    
        # Let's apply boundary conditions
        x = np.linspace(-1, 1, 50)
        t = np.linspace(0.01, 1, 20)
        X, T = np.meshgrid(x, t)
        X_flat = X.flatten()
        T_flat = T.flatten()
        D_values = [0.1, 0.2, 0.3, 0.4]
        N = X_flat.shape[0]
\end{verbatim} 
\end{center}


Based on the code provided, one can observe that the model is trained to predict the values of the diffusion equation at different points in space and time, for a range of diffusion coefficients. The training process involved minimizing the difference between the predicted values and the actual values of the diffusion equation using mean squared error loss. 

After training, the model is used to predict the values of the diffusion equation for each value of the diffusion coefficient in the range of D\_values. The predicted values are then plotted using contour plots, with the x-axis representing position, the y-axis representing time, and the color indicating the value of the diffusion equation.

As stated in the methods section, the homogeneous Neumann's boundary conditions are assumed (refer to the \eqref{bc1}). These boundary conditions state that the normal derivative of the solution at the boundary is zero. In this code, the boundaries of the domain are implicitly assumed to be reflective, which means that the diffusion equation at the boundaries is equal to zero. This assumption is necessary to avoid numerical errors due to the finite size of the domain.
 
% There for we will use the Equation (\ref{eq15}) as our main equation for the  PiNN's model. 